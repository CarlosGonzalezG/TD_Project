{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Indicar el camino hacia los archivos \"projects.xlsx\" y \"paises.xlsx"
      ],
      "metadata": {
        "id": "1HI4Q35VoQr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_PROJECTS= \"/content/projects.xlsx\" #projects.xlsx\n",
        "PATH_PAISES= \"/content/paises.xlsx\" #paises.xlsx"
      ],
      "metadata": {
        "id": "f93VpX9Gn-IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importando librerías y archivo principal"
      ],
      "metadata": {
        "id": "F2uedj6Jkblv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8KTLcBwozDe",
        "outputId": "cb472cdb-88b7-4433-afd1-4d382da59730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.util import ngrams\n",
        "import gensim.corpora as corpora\n",
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import phrases, word2vec\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import inaugural, stopwords\n",
        "from wordcloud import STOPWORDS,WordCloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiA0IGhBprhv"
      },
      "source": [
        "Importando el xlsx propocionado (\"projects.xlsx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H237FMqupCD8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "7ad47200-00b4-4937-c65d-44ade715bf1f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2916f22e546d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_PROJECTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1650\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1653\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1526\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/projects.xlsx'"
          ]
        }
      ],
      "source": [
        "df = pd.read_excel(PATH_PROJECTS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando el xlsx auxiliar (paises2.xlsx)"
      ],
      "metadata": {
        "id": "eLbbYS-KmDGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iso_codes = pd.read_excel(PATH_PAISES)"
      ],
      "metadata": {
        "id": "nSEn-ZEJmCgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9tTbfPr0emU"
      },
      "source": [
        "#**Preprocesado de textos**#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YeWKkG6w2pU"
      },
      "outputs": [],
      "source": [
        "df_a_procesar1 = pd.DataFrame(df)\n",
        "seleccion_columnas = df_a_procesar1[[\"title\",\"summary\"]]\n",
        "new_df = seleccion_columnas.copy()\n",
        "new_df[\"texto_analizar\"] = new_df[[\"summary\", \"title\"]].apply(\" \".join, axis=1)\n",
        "#variable que usaremos en las tareas de regresión\n",
        "y = df.ecMaxContribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGByM82IKx6E"
      },
      "source": [
        "\n",
        "#**Pipeline**#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJdpc10XQa4Z"
      },
      "outputs": [],
      "source": [
        "#palabras que se repiten mucho y meten ruido innecesario\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "stop_words.extend(['new','project'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-mLCJokRHXJ"
      },
      "outputs": [],
      "source": [
        "def pipeline(text):\n",
        "    tokenization = word_tokenize(str(text).replace(\"'\", \"\").lower())\n",
        "    # Para eliminar todos los tokens alfanuméricos\n",
        "    punctuaction = [i for i in tokenization if i.isalpha()]\n",
        "    text = [WordNetLemmatizer().lemmatize(word) for word in punctuaction]\n",
        "    stop_w = [j for j in text if j not in stop_words]\n",
        "    return \" \".join(stop_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYmzznQADgmb"
      },
      "outputs": [],
      "source": [
        "new_df[\"texto_analizar\"] = new_df[\"texto_analizar\"].apply(pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbp-76KXU1Qi"
      },
      "source": [
        "# Extracción de temáticas LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQcFBXkrWYr-"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXotXm0eWEQk"
      },
      "outputs": [],
      "source": [
        "corpus_clean = new_df[\"texto_analizar\"].map(lambda x: x.split(' '))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzvma2suWOXi"
      },
      "source": [
        "Creando un diccionario con la lista de tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vvYZb2EVvRQ"
      },
      "outputs": [],
      "source": [
        "D = gensim.corpora.Dictionary(corpus_clean)\n",
        "n_tokens = len(D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGKnq6gEWby1"
      },
      "source": [
        "Convirtiendo una lista de tokens en una bag-of-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHrDgccyWCxL"
      },
      "outputs": [],
      "source": [
        "corpus_bow = [D.doc2bow(doc) for doc in corpus_clean]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WGnDeVDVgLx"
      },
      "source": [
        "## Evaluación del modelo LDA usando la coherencia ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1OZnpOzOW1v"
      },
      "outputs": [],
      "source": [
        "from gensim.models import CoherenceModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí está comentada la función que usamos para poder seleccionar el número de tópicos, la elección se toma en base a la coherencia"
      ],
      "metadata": {
        "id": "QMJceI2e2f12"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wlyt-dAoOEJ-"
      },
      "outputs": [],
      "source": [
        "\"\"\"def calculate_coherence_score(n, alpha, beta):\n",
        "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus_bow,\n",
        "                                           id2word=D,\n",
        "                                           num_topics=n,\n",
        "                                           alpha=alpha,\n",
        "                                           per_word_topics=True,\n",
        "                                           eta = beta)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus_clean, dictionary=D, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    return coherence_lda\n",
        "\n",
        "#lista de hiperparámetros\n",
        "n_topics = [40,50,60]\n",
        "alpha_list = ['symmetric',0.5]\n",
        "beta_list = ['auto',0.1,0.2]\n",
        "\n",
        "for n in n_topics:\n",
        "    for alpha in alpha_list:\n",
        "        for beta in beta_list:\n",
        "            coherence_score = calculate_coherence_score(n, alpha, beta)\n",
        "            print(f\"n : {n} ; alpha : {alpha} ; beta : {beta} ; Score : {coherence_score}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSjXOYp9W2sh"
      },
      "outputs": [],
      "source": [
        "num_topics = 60\n",
        "beta = 0.2\n",
        "ldag = gensim.models.ldamodel.LdaModel(corpus=corpus_bow, id2word=D, num_topics=num_topics,eta = beta, alpha='symmetric')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Representación de tópicos#"
      ],
      "metadata": {
        "id": "fWMgYWSNzTMh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ-yCNuTX92W"
      },
      "outputs": [],
      "source": [
        "def doc_to_lda_features(lda_model, document):\n",
        "    \"\"\" Transforms a bag of words document to features.\n",
        "    It returns the proportion of how much each topic was\n",
        "    present in the document.\n",
        "    \"\"\"\n",
        "    topic_importances = ldag.get_document_topics(document, minimum_probability=0)\n",
        "    topic_importances = np.array(topic_importances)\n",
        "    return topic_importances[:,1]\n",
        "\n",
        "new_df['lda_features'] = list(map(lambda doc:doc_to_lda_features(ldag, doc),corpus_bow))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3jbPMX8Z3vZ"
      },
      "outputs": [],
      "source": [
        "# Representación de los tópicos más relevantes\n",
        "ldag.print_topics(num_topics=-1, num_words=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Búsqueda de hiperparámetros para los modelos de Regresión#"
      ],
      "metadata": {
        "id": "EqBD7U8m-HMn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g9azH4Eqi04"
      },
      "outputs": [],
      "source": [
        "#parameters = {'n_estimators':[500,600,700,1000],'learning_rate':[0.1,0.01],'max_depth':[5,7],'gamma':[0.01]}\n",
        "#gsearch = GridSearchCV(estimator=xgb_cv, param_grid = parameters ,cv=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLr7YuDX6WmX"
      },
      "source": [
        "# Análisis tarea regresión TFIDF#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU7iypy26Yyv"
      },
      "outputs": [],
      "source": [
        "#convertimos un conjunto de documentos en una matriz términos-frecuencia-inversa\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(new_df[\"texto_analizar\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKzLus-f9SO1"
      },
      "outputs": [],
      "source": [
        "xgb_cv  = XGBRegressor(n_estimator=500,objective ='reg:squarederror')\n",
        "record_scores = cross_val_score(xgb_cv, X_tfidf, y,cv=5)\n",
        "print(\"Scores:\", record_scores)\n",
        "print(f'Mean accuracy: {np.mean(record_scores):.4f} (std: +/- {np.std(record_scores):.4f})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7TKjLy5MoVS"
      },
      "source": [
        "#Análisis tarea regresión LDA#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TseLpCTIsU7"
      },
      "outputs": [],
      "source": [
        "X_lda = np.array(list(map(np.array, new_df['lda_features'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhqOexaG-0XU"
      },
      "outputs": [],
      "source": [
        "xgb_cv  = XGBRegressor(n_estimator=1000)\n",
        "record_scores = cross_val_score(xgb_cv, X_lda, y,cv=5)\n",
        "print(\"Scores:\", record_scores)\n",
        "print(f'Mean accuracy: {np.mean(record_scores):.4f} (std: +/- {np.std(record_scores):.4f})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhOI25UlKFM5"
      },
      "source": [
        "#Tarea de regresión LDA añadiendo metadatos#"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "onehotencoder = OneHotEncoder(categories='auto')\n",
        "x_oh = onehotencoder.fit_transform(df.fundingScheme.values.reshape(-1,1)).toarray()"
      ],
      "metadata": {
        "id": "NsLEIrlG4mFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DZyoFeJX4s2"
      },
      "outputs": [],
      "source": [
        "df_a_procesar = pd.DataFrame(df)\n",
        "seleccion_columnas = df_a_procesar[[\"title\",\"summary\"]]\n",
        "new_df_meta = seleccion_columnas.copy()\n",
        "new_df_meta[\"texto_analizar\"] = new_df_meta[[\"title\",\"summary\"]].apply(\" \".join, axis=1)\n",
        "new_df_meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz5tQImNhrF0"
      },
      "outputs": [],
      "source": [
        "new_df_meta[\"texto_analizar\"] = new_df_meta[\"texto_analizar\"].apply(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qM2zKJcNY1c3"
      },
      "outputs": [],
      "source": [
        "corpus_clean_meta = new_df_meta[\"texto_analizar\"].map(lambda x: x.split(' '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa7h7dJhY1zx"
      },
      "outputs": [],
      "source": [
        "D_meta = gensim.corpora.Dictionary(corpus_clean_meta)\n",
        "n_tokens = len(D_meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEM1sLsHh6WS"
      },
      "outputs": [],
      "source": [
        "corpus_bow_meta = [D.doc2bow(doc) for doc in corpus_clean_meta]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQQkgS7niDMW"
      },
      "outputs": [],
      "source": [
        "num_topics = 60\n",
        "ldag = gensim.models.ldamodel.LdaModel(corpus=corpus_bow_meta, id2word=D_meta, num_topics=num_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bh0Zey5iIfb"
      },
      "outputs": [],
      "source": [
        "new_df_meta['lda_features'] = list(map(lambda doc:doc_to_lda_features(ldag, doc),corpus_bow_meta))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2seGqdiKLP-"
      },
      "outputs": [],
      "source": [
        "X_lda = np.array(list(map(np.array, new_df['lda_features'])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_meta_LDA = np.concatenate([X_lda,x_oh], axis=1)"
      ],
      "metadata": {
        "id": "RGfLpu-n1pQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78brf83JSkW4"
      },
      "outputs": [],
      "source": [
        "xgb_meta_LDA  = XGBRegressor(n_estimator=500)\n",
        "record_scores = cross_val_score(xgb_meta_LDA,X_meta_LDA,y,cv=5)\n",
        "print(\"Scores:\", record_scores)\n",
        "print(f'Mean accuracy: {np.mean(record_scores):.4f} (std: +/- {np.std(record_scores):.4f})')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tarea de regresión TF-IDF añadiendo metadatos#"
      ],
      "metadata": {
        "id": "EaNhXHbq1Vf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_a_procesar = pd.DataFrame(df)\n",
        "seleccion_columnas = df_a_procesar[[\"title\",\"summary\"]]\n",
        "new_df_meta_TF = seleccion_columnas.copy()\n",
        "new_df_meta_TF[\"text\"] = new_df_meta_TF[[\"title\",\"summary\"]].apply(\" \".join, axis=1)"
      ],
      "metadata": {
        "id": "3ejFAa-h1Uu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df_meta_TF['lda_features'] = list(map(lambda doc:doc_to_lda_features(ldag, doc),corpus_bow_meta))"
      ],
      "metadata": {
        "id": "c6de4-ia8RUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_TF = np.array(list(map(np.array, new_df['lda_features'])))"
      ],
      "metadata": {
        "id": "vSjHKWi88luS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf_meta = vectorizer.fit_transform(new_df_meta_TF[\"text\"])\n",
        "y = df.ecMaxContribution"
      ],
      "metadata": {
        "id": "nHLGz08M9bFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_meta_TF = hstack([X_tfidf_meta, x_oh])"
      ],
      "metadata": {
        "id": "-ZHwx9_38x23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_cv  = XGBRegressor(n_estimator=500,objective ='reg:squarederror')\n",
        "record_scores = cross_val_score(xgb_cv, X_meta_TF, y,cv=5)\n",
        "print(\"Scores:\", record_scores)\n",
        "print(f'Mean accuracy: {np.mean(record_scores):.4f} (std: +/- {np.std(record_scores):.4f})')"
      ],
      "metadata": {
        "id": "E8ke6w0l9kZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extension: Usando bigramas en el algortimo LDA#"
      ],
      "metadata": {
        "id": "0GKxCEje-uXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construyendo bigramas\n",
        "bigram = gensim.models.Phrases(new_df[\"texto_analizar\"], min_count=5, threshold=100) # higher threshold fewer phrases."
      ],
      "metadata": {
        "id": "YgC5lwn8-ssF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_clean = new_df[\"texto_analizar\"].map(lambda x: x.split(' '))"
      ],
      "metadata": {
        "id": "J8FDV_xF--3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forma alternativa y más rápida de crear bigramas\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)"
      ],
      "metadata": {
        "id": "Zy0hFFcK-_8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions for bigrams\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]"
      ],
      "metadata": {
        "id": "ia5KtvnL-_58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])"
      ],
      "metadata": {
        "id": "toyusGwR-_3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "metadata": {
        "id": "U1yAD1uZ-_1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_words_bigrams = make_bigrams(corpus_clean)"
      ],
      "metadata": {
        "id": "5oH-1YL9-_yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
      ],
      "metadata": {
        "id": "Wg9qYx70-_s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creando el diccionario\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# creando el corpus\n",
        "texts = data_lemmatized\n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ],
      "metadata": {
        "id": "Kw_jtFFa-_j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = gensim.models.LdaModel(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=60,\n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=10,\n",
        "                                       per_word_topics=True)"
      ],
      "metadata": {
        "id": "0qGwnWin-_O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[\"bigramas\"]=data_words_bigrams"
      ],
      "metadata": {
        "id": "wJDQqeXQ_Q98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelo de regresión usando bigramas#\n"
      ],
      "metadata": {
        "id": "O1MdJQR5_XRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def doc_to_lda_features(lda_model, document):\n",
        "    \"\"\" Transforms a bag of words document to features.\n",
        "    It returns the proportion of how much each topic was\n",
        "    present in the document.\n",
        "    \"\"\"\n",
        "    topic_importances = lda_model.get_document_topics(document, minimum_probability=0)\n",
        "    topic_importances = np.array(topic_importances)\n",
        "    return topic_importances[:,1]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "metadata": {
        "id": "xTV872Ji_Q7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df['bigramas'] = list(map(lambda doc:doc_to_lda_features(lda_model, doc),corpus))"
      ],
      "metadata": {
        "id": "YwDHpOjN_Q4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_lda = np.array(list(map(np.array, new_df['bigramas'])))\n",
        "y = df.ecMaxContribution"
      ],
      "metadata": {
        "id": "_1OReqJh_Qu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_cv  = XGBRegressor(n_estimator=500)\n",
        "record_scores = cross_val_score(xgb_cv, X_lda, y,cv=5)\n",
        "print(\"Scores:\", record_scores)\n",
        "print(f'Mean accuracy: {np.mean(record_scores):.4f} (std: +/- {np.std(record_scores):.4f})')"
      ],
      "metadata": {
        "id": "761nXzwC_jWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhkTkvNorwB8"
      },
      "source": [
        "# DASHBOARD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZr6IxVgryqy"
      },
      "source": [
        "## Instalación e importación de paquetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBG5k01pr6En"
      },
      "outputs": [],
      "source": [
        "!pip install jupyter-dash -q\n",
        "!pip install dash-cytoscape -q\n",
        "!pip install pycountry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NulH1JbNr66E"
      },
      "outputs": [],
      "source": [
        "from jupyter_dash import JupyterDash  # pip install dash\n",
        "import dash_cytoscape as cyto  # pip install dash-cytoscape==0.2.0 or higher\n",
        "import dash_html_components as html\n",
        "import dash_core_components as dcc\n",
        "from dash.dependencies import Input, Output\n",
        "import pandas as pd  # pip install pandas\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import math\n",
        "from dash import no_update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL6Cd4WPsaTr"
      },
      "source": [
        "## Creación de nuesvos dataframes\n",
        "Creación de dataframes auxiliares que eliminen campos que no vamos a usar para que sea más rápido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MIhtjOEsZu8"
      },
      "outputs": [],
      "source": [
        "new_df2 = new_df\n",
        "new_df2['lda_features_max'] = new_df2['lda_features'].apply(np.argmax)\n",
        " #Añado fechas de inicio y final\n",
        "new_df2['startDate'] = df_a_procesar1['startDate']\n",
        "new_df2['endDate'] = df_a_procesar1['endDate']\n",
        "#Añado el pais coordinador\n",
        "new_df2['coordinatorCountry'] = df_a_procesar1['coordinatorCountry']\n",
        "new_df2['coordinatorCountry'] = new_df2['coordinatorCountry'].str.split(';').str[0].dropna()\n",
        "\n",
        "# Cargamos el archivo de correspondencias que asocia códigos ISO de dos letras con códigos ISO de tres letras\n",
        "iso_codes = iso_codes[[\"name\",\"iso2\",\"iso3\"]]\n",
        "\n",
        "\n",
        "counts_lda_features_max = new_df2['lda_features_max'].value_counts()\n",
        "df_lda_features_max = new_df2[[\"lda_features_max\", \"coordinatorCountry\"]]\n",
        "df_lda_features_max = df_lda_features_max.merge(iso_codes, left_on='coordinatorCountry', right_on='iso2')\n",
        "df_lda_features_max = df_lda_features_max[[\"lda_features_max\",\"name\", \"iso3\"]]\n",
        "\n",
        "\n",
        "counts_coordinatorCountry = new_df2['coordinatorCountry'].value_counts()\n",
        "#df_counts_lda = pd.DataFrame({'lda_features_max': counts_lda_features_max.index, 'count':counts_lda_features_max.values})\n",
        "df_counts_lda=new_df2\n",
        "df_counts_lda['startDate'] = pd.to_datetime(new_df2['startDate'])\n",
        "df_counts_lda['startDateY']=df_counts_lda['startDate'].dt.strftime('%Y')\n",
        "df_counts_lda['startDateM']=df_counts_lda['startDate'].dt.strftime('%M')\n",
        "df_counts_lda['startDateD']=df_counts_lda['startDate'].dt.strftime('%D')\n",
        "\n",
        "df_counts_lda['endDate'] = pd.to_datetime(new_df2['endDate'])\n",
        "df_counts_lda['endDateY']=df_counts_lda['endDate'].dt.strftime('%Y')\n",
        "df_counts_lda['endDateM']=df_counts_lda['endDate'].dt.strftime('%M')\n",
        "df_counts_lda['sendDateD']=df_counts_lda['endDate'].dt.strftime('%D')\n",
        "\n",
        "counts_3 = df_counts_lda.groupby(['startDateY', 'lda_features_max']).size().reset_index(name='count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoHuldL1s94t"
      },
      "source": [
        "## Creación de figuras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwaqUT_QtEh2"
      },
      "source": [
        "Gráfico de barras que muestra el tópico mayoritario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJQlLGKFtD6N"
      },
      "outputs": [],
      "source": [
        "fig_LDA_features = go.Figure(data=[go.Bar(x=counts_lda_features_max.index, y=counts_lda_features_max.values)])\n",
        "fig_LDA_features.update_layout(xaxis_title='Tópico LDA mayoritario', yaxis_title='Frecuencias', title='Histograma de tópicos LDA mayoritarios')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOdnlJUAtKy-"
      },
      "source": [
        "Gráfico de barras que muestra el tópico mayoritario diferenciando por años"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtCXet_OtQ_u"
      },
      "outputs": [],
      "source": [
        "fig_LDA_features_year = px.bar(counts_3, x=\"lda_features_max\", y=\"count\", color=\"startDateY\")\n",
        "fig_LDA_features_year.update_layout(barmode='stack', xaxis={'categoryorder':'category ascending'}, xaxis_title='Main LDA topic', yaxis_title='Frequency', legend_title=\"Start year\", title =\"Main LDA features\")\n",
        "fig_LDA_features_year.update_layout(hovermode=\"closest\")\n",
        "#hovertemp= \"<b>Start Year: </b><br>\"\n",
        "hovertemp = \"LDA feature: %{x} <br>\"\n",
        "hovertemp += \"Counts: %{y}\"\n",
        "fig_LDA_features_year.update_traces(hovertemplate=hovertemp)\n",
        "fig_LDA_features_year.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkGXEXfntme4"
      },
      "source": [
        "Histograma que muestra el número de veces que un pais es coordinador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28FKOENqtuQk"
      },
      "outputs": [],
      "source": [
        "fig_coordinatorCountry_hist = go.Figure(data=[go.Bar(x=counts_coordinatorCountry.index, y=counts_coordinatorCountry.values)])\n",
        "fig_coordinatorCountry_hist.update_layout(xaxis_title='Country', yaxis_title='Coordinated projects', title= 'Coordinator country')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3-ygfLmtRuF"
      },
      "source": [
        "Mapa que muestra los proyectos que han sido coordinados por pais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gl1pQEfthqE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Contamos el número de veces que se repite cada código ISO en el campo 'coordinatorCountry'\n",
        "counts = new_df2['coordinatorCountry'].value_counts()\n",
        "\n",
        "# Creamos un dataframe con los resultados de la contabilización\n",
        "df_coordinatorCountry = pd.DataFrame({'iso_code': counts.index, 'count': counts.values})\n",
        "\n",
        "# Añadimos a nuestro dataframe la columna 'iso_code_3' que contiene los códigos ISO de tres letras de Plotly\n",
        "df_coordinatorCountry = df_coordinatorCountry.merge(iso_codes, left_on='iso_code', right_on='iso2')\n",
        "df_coordinatorCountry = df_coordinatorCountry[[\"name\",\"iso3\", \"count\"]]\n",
        "\n",
        "fig_coordinatorCountry_map = go.Figure(\n",
        "    data = {\n",
        "        'type':'choropleth',\n",
        "        'locations':df_coordinatorCountry['name'],\n",
        "        'locationmode':'country names',\n",
        "        'colorscale':'Portland',\n",
        "        'z':df_coordinatorCountry['count'],\n",
        "        'colorbar':{'title':'Coordinated projects'},\n",
        "        'marker': {\n",
        "            'line': {\n",
        "                'color':'rgb(255,255,255)',\n",
        "                'width':2\n",
        "            }\n",
        "        }\n",
        "\n",
        "    },\n",
        "    layout = {\n",
        "      'geo':{\n",
        "          'projection':{\n",
        "              'scale':5,\n",
        "              'rotation':{\n",
        "                  'lat':45,\n",
        "                  'lon':15\n",
        "              },\n",
        "              #'type':'bromley'  # default is 'equirectangular'\n",
        "              #'type':'cylindrical stereographic'  # default is 'equirectangular'\n",
        "              #'type':'equirectangular'  # default is 'equirectangular'\n",
        "              #'type':'orthographic'  # default is 'equirectangular'\n",
        "              'type':'natural earth'  # default is 'equirectangular'\n",
        "\n",
        "\n",
        "          },\n",
        "          'scope':'world',\n",
        "          'showcountries': True,\n",
        "          'showcoastlines': True\n",
        "      }\n",
        "    })\n",
        "fig_coordinatorCountry_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3blJ28i726D"
      },
      "outputs": [],
      "source": [
        "fig_coordinatorCountry_hist_2 = go.Figure(data=[go.Bar(x=counts_coordinatorCountry.index, y=counts_coordinatorCountry.values)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYOC90-xYbvG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "topico=40\n",
        "v0=ldag.print_topics(num_topics=-1,num_words=10)[topico][1]\n",
        "aux = v0.split('+')\n",
        "words=[]\n",
        "prob=[]\n",
        "for token in aux:\n",
        "  words.append(token.split('*'))\n",
        "df_words=pd.DataFrame(words)\n",
        "df_words=df_words.rename(columns={0: 'prob', 1:'word'})\n",
        "df_words['prob']=df_words['prob']\n",
        "df_words['prob']=df_words['prob'].astype(float)*1000\n",
        "df_words['word']=df_words['word'].str.replace('\"','')\n",
        "\n",
        "\n",
        "colors = [px.colors.DEFAULT_PLOTLY_COLORS[random.randrange(1, 10)] for i in range(30)]\n",
        "data = go.Scatter(x=[random.random() for i in range(100)],\n",
        "                  y=random.choices(range(100), k=10),\n",
        "                 mode='text',\n",
        "                 text=df_words['word'],\n",
        "                 marker={'opacity':0.45},\n",
        "                 textfont={'size': df_words['prob']*2,\n",
        "                           'color': colors},\n",
        "                 hoverinfo='none')\n",
        "layout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},\n",
        "                    'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}}\n",
        "                   )\n",
        "nube_palabras = go.Figure(data=[data], layout=layout)\n",
        "\n",
        "nube_palabras.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicación de DASH"
      ],
      "metadata": {
        "id": "WYFTTkV2qlAs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqI3drqu6mBi"
      },
      "outputs": [],
      "source": [
        "unique_values_hist = df_lda_features_max['lda_features_max'].unique()\n",
        "unique_values_hist.sort()\n",
        "options = [{'label':'Feature '+ str(value), 'value': value} for value in unique_values_hist]\n",
        "\n",
        "\n",
        "app = JupyterDash(__name__)\n",
        "\n",
        "app.layout = html.Div(children=[\n",
        "   html.Div(children=[\n",
        "     html.H1(children= \"DASHBOARD\", style={\n",
        "         'textAlign': 'center'\n",
        "     }),\n",
        "     html.H5(children= 'Carlos González Gamella: 100364132'),\n",
        "     html.H5(children='Andrés Grijalba Peña: 100383705')\n",
        "   ]),\n",
        "   # elements from the top of the page\n",
        "   html.Div([html.H1(children='Main LDA feature per year'),\n",
        "             html.H4(\"Select/deselect a year. You can double click on a year to show just that year\"),\n",
        "   #html.Div(children='''Dash: First graph.'''),\n",
        "   dcc.Graph(id=\"lDA_year\", figure=fig_LDA_features_year)\n",
        "   ]),\n",
        "\n",
        "  html.H1(\"Map and chart showing LDA main features\"),\n",
        "  html.H4(\"Click on a country to show main LDA features for project coordinated that were coordinated by that country. You can scroll through the glove\"),\n",
        "      html.Div(style={'display': 'flex', 'justify-content': 'space-between'}, children=[\n",
        "        dcc.Graph(id=\"bar-chart\"),\n",
        "        dcc.Graph(id=\"mapa\", figure=fig_coordinatorCountry_map)\n",
        "      ]),\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   # New Div for all elements in the new 'row' of the page\n",
        "   html.Div(className='row', children=[\n",
        "    html.H1(\"Coordinator country by main LDA topic\"),\n",
        "    html.H4(\"Select o search a country from the dorpdown list\"),\n",
        "    dcc.Dropdown(id='dropdown', options=options, value=unique_values_hist[0], searchable=True),\n",
        "    html.Div(style={'display': 'flex', 'justify-content': 'space-between'}, children=[\n",
        "      dcc.Graph(id='histograma', figure=fig_coordinatorCountry_hist_2, style={'width': '100%'}),\n",
        "      dcc.Graph(id='nube_palabras', figure=nube_palabras, style={'width': '100%'})\n",
        "      ])\n",
        "    ]),\n",
        "])\n",
        "\n",
        "\n",
        "@app.callback(Output(\"bar-chart\", \"figure\"), [Input(\"mapa\", \"clickData\")])\n",
        "def update_chart(click_data):\n",
        "    # Si no se ha hecho clic en el mapa, devuelve un gráfico vacío\n",
        "    if click_data is None or \"points\" not in click_data:\n",
        "        df_filtered = df_lda_features_max\n",
        "    else:\n",
        "      country_code = click_data[\"points\"][0][\"location\"]\n",
        "      df_filtered = df_lda_features_max[df_lda_features_max[\"name\"] == country_code]\n",
        "    # Cuenta las veces que se repite lda_features_max en el dataframe filtrado\n",
        "    print(df_filtered)\n",
        "    counts = df_filtered[\"lda_features_max\"].value_counts()\n",
        "    # Devuelve el gráfico de barras con los datos y la configuración\n",
        "    return {\n",
        "        \"data\": [{\"x\": counts.index, \"y\": counts.values, \"type\": \"bar\"}],\n",
        "        \"layout\": {\"title\": f\"LDA features for {country_code or 'every country'}\"}\n",
        "    }\n",
        "\n",
        "@app.callback(\n",
        "    Output(component_id='histograma', component_property='figure'),\n",
        "    Output(component_id='nube_palabras', component_property='figure'),\n",
        "    Input(component_id='dropdown', component_property='value')\n",
        ")\n",
        "def update_figure(selected_value):\n",
        "    #fig_coordinatorCountry_map2 = actualiza_mapa(selected_value)\n",
        "\n",
        "  filtered_df = df_lda_features_max[df_lda_features_max['lda_features_max'] == selected_value]\n",
        "  counts_coordinatorCountry_2 = filtered_df['iso3'].value_counts()\n",
        "  fig_coordinatorCountry_hist_2 = go.Figure(data=[go.Bar(x=counts_coordinatorCountry_2.index, y=counts_coordinatorCountry_2.values)])\n",
        "  fig_coordinatorCountry_hist_2.update_layout(xaxis_title='Países', yaxis_title='Frecuencias', title= 'Pais coordinador')\n",
        "\n",
        "  v0=ldag.print_topics(num_topics=-1)[selected_value][1]\n",
        "  aux = v0.split('+')\n",
        "  words=[]\n",
        "  prob=[]\n",
        "  for token in aux:\n",
        "    words.append(token.split('*'))\n",
        "  df_words=pd.DataFrame(words)\n",
        "  df_words=df_words.rename(columns={0: 'prob', 1:'word'})\n",
        "  df_words['prob']=df_words['prob']\n",
        "  df_words['prob']=df_words['prob'].astype(float)*1000\n",
        "  df_words['word']=df_words['word'].str.replace('\"','')\n",
        "\n",
        "\n",
        "  colors = [px.colors.DEFAULT_PLOTLY_COLORS[random.randrange(1, 10)] for i in range(30)]\n",
        "  data = go.Scatter(x=[random.random() for i in range(100)],\n",
        "                  y=random.choices(range(100), k=10),\n",
        "                  mode='text',\n",
        "                  text=df_words['word'],\n",
        "                  marker={'opacity':0.45},\n",
        "                  textfont={'size': df_words['prob']*1.25,\n",
        "                            'color': colors},\n",
        "                  hoverinfo='none')\n",
        "  layout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},\n",
        "                      'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}}\n",
        "                    )\n",
        "  nube_palabras = go.Figure(data=[data], layout=layout)\n",
        "\n",
        "\n",
        "\n",
        "  return fig_coordinatorCountry_hist_2, nube_palabras\n",
        "\n",
        "app.run_server(debug=True, mode='inline')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}